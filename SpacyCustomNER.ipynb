{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import docx\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install docx\n",
    "# !pip install exceptions\n",
    "#!pip install --user exceptions\n",
    "# !pip install [--user] pyception\n",
    "# !pip install exception\n",
    "# !pip install python-docx\n",
    "# !pip install lexnlp\n",
    "# from lexnlp.extract.en.addresses import address_features\n",
    "# !pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Aggrement Value</th>\n",
       "      <th>Aggrement Start Date</th>\n",
       "      <th>Aggrement End Date</th>\n",
       "      <th>Renewal Notice (Days)</th>\n",
       "      <th>Party One</th>\n",
       "      <th>Party Two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>404_Sai Sadan_Rental Agreement</td>\n",
       "      <td>15500</td>\n",
       "      <td>26.06.2016</td>\n",
       "      <td>31.04.2017</td>\n",
       "      <td>30</td>\n",
       "      <td>Mr. RK Senthil Kumar</td>\n",
       "      <td>Mr. Sandipan Nandy Mazumdar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bangalore Agreement_New</td>\n",
       "      <td></td>\n",
       "      <td>01.05.2015</td>\n",
       "      <td>28.02.2018</td>\n",
       "      <td>15</td>\n",
       "      <td>MR AMAR ARUN WAGH</td>\n",
       "      <td>MR SANDEEP DUTTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Draft lease Eternia June 1 2017  to  - April 3...</td>\n",
       "      <td>42500</td>\n",
       "      <td>01.06.2017</td>\n",
       "      <td>30.04.2018</td>\n",
       "      <td>30</td>\n",
       "      <td>Mrs. Meera Garge</td>\n",
       "      <td>Mr. Sameer Khanna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rent Agreement Bengaluru</td>\n",
       "      <td>15000</td>\n",
       "      <td>01.05.2016</td>\n",
       "      <td>31.04.2017</td>\n",
       "      <td>60</td>\n",
       "      <td>Mrs. Usha Tiwari</td>\n",
       "      <td>Mr. Vijesh Bhaktha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rental Agreement_Sanjeevini Pebbles</td>\n",
       "      <td>19000</td>\n",
       "      <td>01.08.2017</td>\n",
       "      <td>31.05.2018</td>\n",
       "      <td>30</td>\n",
       "      <td>Mr.Dhannatej Vaddadi</td>\n",
       "      <td>Mr. Sandeep Dutta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RentalAgreement_Bangalore</td>\n",
       "      <td>20000</td>\n",
       "      <td>1.08.2016</td>\n",
       "      <td>31.05.2017</td>\n",
       "      <td>30</td>\n",
       "      <td>Raghuram Sagar</td>\n",
       "      <td>Sunil Thomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RentalAgreement_KKR</td>\n",
       "      <td>8300</td>\n",
       "      <td>01.05.2014</td>\n",
       "      <td>31.03.2015</td>\n",
       "      <td>30</td>\n",
       "      <td>Amitabh Reddy</td>\n",
       "      <td>SrikanthPai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>residential_sample_copy</td>\n",
       "      <td>26000</td>\n",
       "      <td>22.05.2016</td>\n",
       "      <td>21.03.2018</td>\n",
       "      <td>30</td>\n",
       "      <td>Gauravkumar Kate</td>\n",
       "      <td>Makarand Kate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6683127-House-Rental-Contract-GERALDINE-GALINA...</td>\n",
       "      <td>6500</td>\n",
       "      <td>20.05.2007</td>\n",
       "      <td>20.05.2008</td>\n",
       "      <td>15</td>\n",
       "      <td>Antonio Levy S. Ingles, Jr. and/or Mary Rose C...</td>\n",
       "      <td>GERALDINE Q. GALINATO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6683129-House-Rental-Contract-Geraldine-Galina...</td>\n",
       "      <td>6500</td>\n",
       "      <td>20.05.2007</td>\n",
       "      <td>20.05.2008</td>\n",
       "      <td>15</td>\n",
       "      <td>Antonio Levy S. Ingles, Jr. and/or Mary Rose C...</td>\n",
       "      <td>GERALDINE Q. GALINATO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name Aggrement Value  \\\n",
       "0                     404_Sai Sadan_Rental Agreement           15500   \n",
       "1                            Bangalore Agreement_New                   \n",
       "2  Draft lease Eternia June 1 2017  to  - April 3...           42500   \n",
       "3                           Rent Agreement Bengaluru           15000   \n",
       "4                Rental Agreement_Sanjeevini Pebbles           19000   \n",
       "5                          RentalAgreement_Bangalore           20000   \n",
       "6                                RentalAgreement_KKR            8300   \n",
       "7                            residential_sample_copy           26000   \n",
       "8  6683127-House-Rental-Contract-GERALDINE-GALINA...            6500   \n",
       "9  6683129-House-Rental-Contract-Geraldine-Galina...            6500   \n",
       "\n",
       "  Aggrement Start Date Aggrement End Date Renewal Notice (Days)  \\\n",
       "0          26.06.2016          31.04.2017                    30   \n",
       "1           01.05.2015         28.02.2018                    15   \n",
       "2           01.06.2017         30.04.2018                    30   \n",
       "3           01.05.2016         31.04.2017                    60   \n",
       "4           01.08.2017         31.05.2018                    30   \n",
       "5            1.08.2016         31.05.2017                    30   \n",
       "6           01.05.2014         31.03.2015                    30   \n",
       "7           22.05.2016         21.03.2018                    30   \n",
       "8           20.05.2007         20.05.2008                    15   \n",
       "9           20.05.2007         20.05.2008                    15   \n",
       "\n",
       "                                           Party One  \\\n",
       "0                               Mr. RK Senthil Kumar   \n",
       "1                                  MR AMAR ARUN WAGH   \n",
       "2                                  Mrs. Meera Garge    \n",
       "3                                   Mrs. Usha Tiwari   \n",
       "4                              Mr.Dhannatej Vaddadi    \n",
       "5                                     Raghuram Sagar   \n",
       "6                                     Amitabh Reddy    \n",
       "7                                   Gauravkumar Kate   \n",
       "8  Antonio Levy S. Ingles, Jr. and/or Mary Rose C...   \n",
       "9  Antonio Levy S. Ingles, Jr. and/or Mary Rose C...   \n",
       "\n",
       "                      Party Two  \n",
       "0  Mr. Sandipan Nandy Mazumdar   \n",
       "1              MR SANDEEP DUTTA  \n",
       "2            Mr. Sameer Khanna   \n",
       "3           Mr. Vijesh Bhaktha   \n",
       "4            Mr. Sandeep Dutta   \n",
       "5                 Sunil Thomas   \n",
       "6                   SrikanthPai  \n",
       "7                 Makarand Kate  \n",
       "8         GERALDINE Q. GALINATO  \n",
       "9         GERALDINE Q. GALINATO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('TrainingTestSet.csv')\n",
    "df.drop_duplicates(subset='File Name', inplace=True)\n",
    "df.fillna(' ', inplace=True)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = os.getcwd() + '/Training_data/Training_data/'\n",
    "train_ls = os.listdir(train_path)\n",
    "train_ls = list(set(train_ls))\n",
    "len(train_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'216973836-Rental-Agreement-Sample.pdf.docx'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ls[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset(name,t):\n",
    "    pre=r\"[\"+string.punctuation+\"\\s]+\"\n",
    "    pre=r\"([\\s!@#$%^&*()_.';:,\\n]+)\"\n",
    "    #name='vivek'\n",
    "    name='('+name+')'\n",
    "    pattern=pre+name+pre\n",
    "    #print(pattern)\n",
    "    name_offset = [ {'name':a.groups()[1],'start':a.start()+len(a.groups()[0]),'end':a.end()-len(a.groups()[-1]),\n",
    "      'g_offsets':[a.start(),a.end()],'pre_punc':a.groups()[0],'post_punc':a.groups()[-1],\n",
    "      'match_text':a.group()}\n",
    "        for a in\n",
    "    list(re.finditer(pattern,t))]\n",
    "    return name_offset\n",
    "\n",
    "def entities_offset(off_dict):\n",
    "    ent_ls =[]\n",
    "    ent_ls.append(off_dict['start'])\n",
    "    ent_ls.append(off_dict['end'])\n",
    "    #ent_ls.append(entity)\n",
    "    #ent_tup = tuple(ent_ls)\n",
    "    return ent_ls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = train_ls[10]\n",
    "# text = docx.Document(train_path+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'C:\\Users\\vivek.chandra.jha\\Documents\\Rental/Training_data/Training_data/1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0cb6e50a00c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mpara_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\docx\\api.py\u001b[0m in \u001b[0;36mDocument\u001b[1;34m(docx)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m     24\u001b[0m     \u001b[0mdocx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default_docx_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdocx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdocx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mdocument_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_document_part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdocument_part\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent_type\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWML_DOCUMENT_MAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mtmpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"file '%s' is not a Word file, content type is '%s'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\docx\\opc\\package.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mpkg_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mpackage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mUnmarshaller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munmarshal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPartFactory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\docx\\opc\\pkgreader.py\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(pkg_file)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m|\u001b[0m\u001b[0mPackageReader\u001b[0m\u001b[1;33m|\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mloaded\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mphys_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhysPkgReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mcontent_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ContentTypeMap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_xml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent_types_xml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mpkg_srels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_srels_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPACKAGE_URI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\docx\\opc\\phys_pkg.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 raise PackageNotFoundError(\n\u001b[1;32m---> 31\u001b[1;33m                     \u001b[1;34m\"Package not found at '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpkg_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 )\n\u001b[0;32m     33\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# assume it's a stream and pass it to Zip reader to sort out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: Package not found at 'C:\\Users\\vivek.chandra.jha\\Documents\\Rental/Training_data/Training_data/1'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = stopwords.words('english') + list(string.punctuation)\n",
    "train_ls = train_ls[10]\n",
    "for file in train_ls:\n",
    "    print(file)\n",
    "    text = docx.Document(train_path+file)\n",
    "    para_text = ''\n",
    "    for para in text.paragraphs:\n",
    "        para_text = para_text+' '+ para.text\n",
    "        \n",
    "    para_text = para_text.lower()\n",
    "    txt_cleaned = ' '.join([i for i in word_tokenize(para_text.lower()) if i not in stop])\n",
    "    \n",
    "    df_file = df[df['File Name'] == file[:-9]]\n",
    "    name1 = df_file['File Name'].iloc[6]\n",
    "    if len(name1)>2:\n",
    "        name1_off = offset(name1, txt_cleaned.lower())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.matcher import Matcher\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# # Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "# pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "# matcher.add(\"HelloWorld\", None, pattern)\n",
    "\n",
    "# txt = \"Hello, world! Hello world!\"\n",
    "# doc = nlp(txt)\n",
    "# matches = matcher(doc)\n",
    "# for match_id, start, end in matches:\n",
    "#     string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "#     span = doc[start:end]  # The matched span\n",
    "#     print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import en_core_web_sm\n",
    "# from nltk import word_tokenize\n",
    "# nlp = en_core_web_sm.load()\n",
    "# all_stopwords = nlp.Defaults.stop_words\n",
    "# t = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "# text_tokens = word_tokenize(t)\n",
    "# tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {}\n",
    "i = 0\n",
    "for file in train_ls:\n",
    "    try:\n",
    "    #     i=i+1\n",
    "        print(file)\n",
    "        text = docx.Document(train_path+file)\n",
    "    #     print(train_ls[10])\n",
    "        para_text = ''\n",
    "        for para in text.paragraphs:\n",
    "            #print(para.text)\n",
    "        #     para_ls.append(para.text)\n",
    "            para_text = para_text+' '+ para.text\n",
    "            file_dict[file] = list(re.finditer(pattern,para_text))\n",
    "    except:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "# doc = nlp(para_text)\n",
    "# print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flair.models import SequenceTagger\n",
    "# tagger = SequenceTagger.load('ner')\n",
    "# sentence = Sentence(para_text)\n",
    "# tagger.predict(sentence)\n",
    "# for entity in sentence.get_spans('ner'):\n",
    "#     print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_text_s = para_text.split('months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'(\\s)(\\d)?(one)?(two)?(three)?(four)?(\\s)(month)?(months)(\\s)'\n",
    "list(re.finditer(pattern,para_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_text[2939:2949]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
